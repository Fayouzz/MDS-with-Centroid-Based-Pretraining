# Multi-Document Summarization with Centroid-Based Pretraining

This project implements the methodology described in the paper [Multi-Document Summarization with Centroid-Based Pretraining](https://aclanthology.org/2023.acl-short.13.pdf). It focuses on improving multi-document summarization by pretraining models using centroid-based objectives and fine-tuning them for summarization tasks.

## ğŸš€ Features
- **Centroid-Based Pretraining**: Generates pseudo-label summaries from document clusters.
- **Fine-Tuning**: Adapts pretrained models to generate human-like summaries.
- **Evaluation**: Measures performance using ROUGE and other metrics.
- Modular code for reproducibility and experimentation.

## ğŸ“ Repository Structure
```plaintext
â”œâ”€â”€ data/                  # Datasets (raw, processed, centroids)
â”œâ”€â”€ notebooks/             # Jupyter notebooks for exploratory work
â”œâ”€â”€ src/                   # Modular Python scripts for implementation
â”œâ”€â”€ experiments/           # Experiment configurations and logs
â”œâ”€â”€ models/                # Saved models (pretrained and fine-tuned)
â”œâ”€â”€ results/               # Evaluation results, logs, and generated summaries
â”œâ”€â”€ docs/                  # Documentation (methodology, architecture, results)
â”œâ”€â”€ images/                # Visualizations and example outputs
â”œâ”€â”€ README.md              # Project overview and instructions
â”œâ”€â”€ LICENSE                # License for the project
â”œâ”€â”€ .gitignore             # Files and directories to be ignored by Git
â””â”€â”€ requirements.txt       # Python dependencies

## ğŸ› ï¸ Installation
